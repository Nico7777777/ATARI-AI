{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install ale-py\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Load ROMS\n",
    "\n",
    "Get a rom from the bellow link, extract it and then run the `ale-import-roms .` command in the folder with the roms(*implicit directory - project directory*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Supported Games: https://github.com/mgbellemare/Arcade-Learning-Environment/blob/master/docs/games.md\n",
    "from ale_py.roms import Alien\n",
    "from ale_py import ALEInterface, SDL_SUPPORT\n",
    "\n",
    "ale = ALEInterface()\n",
    "# Check to see if we can use UI\n",
    "print(SDL_SUPPORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Get & Set the desired settings\n",
    "ale.setInt(\"random_seed\", 123)\n",
    "# The default is already 0.25, this is just an example\n",
    "ale.setFloat(\"repeat_action_probability\", 0.25)\n",
    "\n",
    "# Check if we can display the screen\n",
    "# For the first set of training better let it without UI/sound\n",
    "if SDL_SUPPORT:\n",
    "    ale.setBool(\"sound\", True)\n",
    "    ale.setBool(\"display_screen\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Load our game\n",
    "ale.loadROM(Alien)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def create_epsilon_greedy_policy(Q, epsilon, num_actions):\n",
    "\t\"\"\"\n",
    "\tCreates an epsilon-greedy policy based\n",
    "\ton a given Q-function and epsilon.\n",
    "\t\n",
    "\tReturns a function that takes the state\n",
    "\tas an input and returns the probabilities\n",
    "\tfor each action in the form of a numpy array\n",
    "\tof length of the action space(set of possible actions).\n",
    "\t\"\"\"\n",
    "\tdef policy_function(state):\n",
    "\n",
    "\t\taction_probabilities = np.ones(num_actions,\n",
    "\t\t\t\tdtype = float) * epsilon / num_actions\n",
    "\t\t\t\t\n",
    "\t\tbest_action = np.argmax(Q[state])\n",
    "\t\taction_probabilities[best_action] += (1.0 - epsilon)\n",
    "\t\treturn action_probabilities\n",
    "\n",
    "\treturn policy_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def from_array_to_hash(x):\n",
    "    return hash(x.tostring())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, discount_factor = 1.0,\n",
    "\t\t\t   alpha = 0.6, epsilon = 0.1):\n",
    "\t\"\"\"\n",
    "\tQ-Learning algorithm: Off-policy TD control.\n",
    "\tFinds the optimal greedy policy while improving\n",
    "\tfollowing an epsilon-greedy policy\"\"\"\n",
    "\t\n",
    "\t# Action value function\n",
    "\t# A nested dictionary that maps\n",
    "\t# state -> (action -> action-value).\n",
    "\t\n",
    "\tlegal_actions = env.getLegalActionSet()\n",
    "\tnum_actions = len(legal_actions)\n",
    "\tprint(num_actions)\n",
    "\tQ = defaultdict(lambda: np.zeros(num_actions))\t\n",
    "\t\n",
    "\t# Create an epsilon greedy policy function\n",
    "\t# appropriately for environment action space\n",
    "\tpolicy = create_epsilon_greedy_policy(Q, epsilon, num_actions)\n",
    "\t\n",
    "\t# For every episode\n",
    "\tfor _ in range(num_episodes):\n",
    "\t\t\n",
    "\t\t# Reset the environment and pick the first action\n",
    "\t\tenv.reset_game()\n",
    "\n",
    "\t\tstate = from_array_to_hash(env.getScreen())\n",
    "\t\tprint(len(env.getScreen()))\n",
    "\t\tprint(len(env.getScreen()[0]))\n",
    "\t\t\n",
    "\t\tfor _ in itertools.count():\n",
    "\t\t\t\n",
    "\t\t\t# get probabilities of all actions from current state\n",
    "\t\t\taction_probabilities = policy(state)\n",
    "\n",
    "\t\t\t# choose action according to\n",
    "\t\t\t# the probability distribution\n",
    "\t\t\taction = np.random.choice(np.arange(\n",
    "\t\t\t\t\tlen(action_probabilities)),\n",
    "\t\t\t\t\tp=action_probabilities)\n",
    "\n",
    "\t\t\t# take action and get reward, transit to next state\n",
    "\t\t\treward = env.act(action)\n",
    "\t\t\tdone = env.game_over()\n",
    "\t\t\tnext_state = from_array_to_hash(env.getScreen())\n",
    "\t\t\t\n",
    "\t\t\t# TD Update\n",
    "\t\t\tbest_next_action = np.argmax(Q[next_state])\t\n",
    "\t\t\ttd_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "\t\t\ttd_delta = td_target - Q[state][action]\n",
    "\t\t\tQ[state][action] += alpha * td_delta\n",
    "\n",
    "\t\t\t# done is True if episode terminated\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\t\t\t\t\n",
    "\t\t\tstate = next_state\n",
    "\t\n",
    "\treturn Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "avail_modes = ale.getAvailableModes()\n",
    "avail_diff = ale.getAvailableDifficulties()\n",
    "\n",
    "print(f\"Number of available modes: {len(avail_modes)}\")\n",
    "print(f\"Number of available difficulties: {len(avail_diff)}\")\n",
    "\n",
    "# Get the list of legal actions\n",
    "leg_act = ale.getLegalActionSet()\n",
    "\n",
    "# NUMBER OF GAMES = no. of MODES x no. of DIFFICULTY levels\n",
    "for mode in avail_modes:\n",
    "    for diff in avail_diff:\n",
    "\n",
    "        ale.setDifficulty(diff)\n",
    "        ale.setMode(mode)\n",
    "        ale.reset_game()\n",
    "        print(f\"Mode {mode} difficulty {diff}:\")\n",
    "        q_learning(ale, 1000)\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "18*210*160*5/(100*60*60) - acesta este un calcul al complexitatii pe care l-am si il voi intreba pe Alex data viitoare"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a55aa93dbdd18667d196012e14dbc6ba031cd307f6972d5be3ce3ad20c92d5dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
